{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Initialize Cerebras client\n",
    "api_key = os.environ.get(\"CEREBRAS_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"CEREBRAS_API_KEY environment variable is not set.\")\n",
    "\n",
    "client = Cerebras(api_key=api_key)\n",
    "\n",
    "# Initialize LangChain memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Function to interact with Cerebras API\n",
    "def get_cerebras_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"llama3.1-8b\",\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "# Create a conversation chain\n",
    "def chat():\n",
    "    print(\"Welcome to the Cerebras Chat! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"/exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Add user input to memory\n",
    "        memory.chat_memory.add_user_message(user_input)\n",
    "\n",
    "        # Get the conversation history from memory\n",
    "        history = memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "        # Generate a prompt with conversation history\n",
    "        prompt = f\"{history}\\nUser: {user_input}\\nAI:\"\n",
    "\n",
    "        # Get response from Cerebras API\n",
    "        ai_response = get_cerebras_response(prompt)\n",
    "\n",
    "        # Add AI response to memory\n",
    "        memory.chat_memory.add_ai_message(ai_response)\n",
    "\n",
    "        # Print AI response\n",
    "        print(f\"AI: {ai_response}\")\n",
    "\n",
    "# Start the chat\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Cerebras Chat! Type '/exit' to end the conversation.\n",
      "Response Object: ChatCompletion(id='chatcmpl-6bf7b910-796e-4a38-ade9-5a83b8aa7050', choices=[Choice(finish_reason='stop', index=0, message=ChoiceMessage(role='assistant', content='Hello, how can I assist you today?', tool_calls=None), logprobs=None)], created=1737920853, model='llama3.1-8b', object='chat.completion', system_fingerprint='fp_6381a6c109', time_info=TimeInfo(completion_time=0.004537883, prompt_time=0.002304832, queue_time=9.4071e-05, total_time=0.008230447769165039, created=1737920853), usage=Usage(completion_tokens=10, prompt_tokens=45, total_tokens=55), service_tier=None)\n",
      "AI: Hello, how can I assist you today?\n",
      "Response Object: ChatCompletion(id='chatcmpl-f8e36301-7b3f-48af-81f7-743f1072cfcf', choices=[Choice(finish_reason='stop', index=0, message=ChoiceMessage(role='assistant', content=\"I'm functioning within normal parameters, thank you for asking. I'm a large language model, so I don't have emotions or feelings like humans do, but I'm ready to provide assistance and help with any questions or topics you'd like to discuss. Is there something specific on your mind, or would you like to talk about something in general?\", tool_calls=None), logprobs=None)], created=1737920864, model='llama3.1-8b', object='chat.completion', system_fingerprint='fp_6381a6c109', time_info=TimeInfo(completion_time=0.032361355, prompt_time=0.003296659, queue_time=8.744e-05, total_time=0.03705716133117676, created=1737920864), usage=Usage(completion_tokens=71, prompt_tokens=64, total_tokens=135), service_tier=None)\n",
      "AI: I'm functioning within normal parameters, thank you for asking. I'm a large language model, so I don't have emotions or feelings like humans do, but I'm ready to provide assistance and help with any questions or topics you'd like to discuss. Is there something specific on your mind, or would you like to talk about something in general?\n",
      "Response Object: ChatCompletion(id='chatcmpl-45df652e-054c-4c29-b382-4141f0a6ac6d', choices=[Choice(finish_reason='stop', index=0, message=ChoiceMessage(role='assistant', content='**Sample FastAPI Code**\\n=======================\\n\\nBelow is a simple example of a FastAPI application that provides a few endpoints for demonstrating its capabilities.\\n\\n### Installation\\n\\nBefore running the code, make sure you have FastAPI and Uvicorn installed. You can install them using pip:\\n\\n```bash\\npip install fastapi uvicorn\\n```\\n\\n### Code\\n\\n```python\\n# app.py\\n\\nfrom fastapi import FastAPI\\nfrom pydantic import BaseModel\\n\\n# Create a FastAPI instance\\napp = FastAPI()\\n\\n# Define a model for data validation\\nclass Item(BaseModel):\\n    name: str\\n    price: float\\n    is_active: bool\\n\\n# Create a route to get a list of items\\n@app.get(\"/items/\")\\nasync def read_items():\\n    return [{\"id\": 1, \"name\": \"Item 1\", \"price\": 10.99}, {\"id\": 2, \"name\": \"Item 2\", \"price\": 5.99}]\\n\\n# Create a route to get a specific item by ID\\n@app.get(\"/items/{item_id}\")\\nasync def read_item(item_id: int):\\n    return {\"id\": item_id, \"name\": \"Item \" + str(item_id), \"price\": 10.99}\\n\\n# Create a route to create a new item\\n@app.post(\"/items/\")\\nasync def create_item(item: Item):\\n    return {\"id\": 3, \"name\": item.name, \"price\": item.price}\\n\\n# Create a route to update an existing item\\n@app.put(\"/items/{item_id}\")\\nasync def update_item(item_id: int, item: Item):\\n    return {\"id\": item_id, \"name\": item.name, \"price\": item.price}\\n\\n# Create a route to delete an item\\n@app.delete(\"/items/{item_id}\")\\nasync def delete_item(item_id: int):\\n    return {\"message\": \"Item \" + str(item_id) + \" deleted\"}\\n```\\n\\n### Running the Code\\n\\nTo run the code, execute the following command in your terminal:\\n\\n```bash\\nuvicorn app:app --reload\\n```\\n\\nYou can now access the API endpoints by visiting `http://localhost:8000/docs` in your web browser. The `--reload` flag allows the server to automatically reload when you make changes to the code.\\n\\n### Example Responses\\n\\nHere are some example responses from the API:\\n\\n* `GET /items/`: Returns a list of items.\\n* `GET /items/1`: Returns a specific item by ID.\\n* `POST /items/`: Creates a new item.\\n* `PUT /items/1`: Updates an existing item.\\n* `DELETE /items/1`: Deletes an item.\\n\\nRemember to replace the item IDs with actual values for updating and deleting items.', tool_calls=None), logprobs=None)], created=1737920889, model='llama3.1-8b', object='chat.completion', system_fingerprint='fp_6381a6c109', time_info=TimeInfo(completion_time=0.257459141, prompt_time=0.007874797, queue_time=8.563e-05, total_time=0.26722264289855957, created=1737920889), usage=Usage(completion_tokens=564, prompt_tokens=152, total_tokens=716), service_tier=None)\n",
      "AI: **Sample FastAPI Code**\n",
      "=======================\n",
      "\n",
      "Below is a simple example of a FastAPI application that provides a few endpoints for demonstrating its capabilities.\n",
      "\n",
      "### Installation\n",
      "\n",
      "Before running the code, make sure you have FastAPI and Uvicorn installed. You can install them using pip:\n",
      "\n",
      "```bash\n",
      "pip install fastapi uvicorn\n",
      "```\n",
      "\n",
      "### Code\n",
      "\n",
      "```python\n",
      "# app.py\n",
      "\n",
      "from fastapi import FastAPI\n",
      "from pydantic import BaseModel\n",
      "\n",
      "# Create a FastAPI instance\n",
      "app = FastAPI()\n",
      "\n",
      "# Define a model for data validation\n",
      "class Item(BaseModel):\n",
      "    name: str\n",
      "    price: float\n",
      "    is_active: bool\n",
      "\n",
      "# Create a route to get a list of items\n",
      "@app.get(\"/items/\")\n",
      "async def read_items():\n",
      "    return [{\"id\": 1, \"name\": \"Item 1\", \"price\": 10.99}, {\"id\": 2, \"name\": \"Item 2\", \"price\": 5.99}]\n",
      "\n",
      "# Create a route to get a specific item by ID\n",
      "@app.get(\"/items/{item_id}\")\n",
      "async def read_item(item_id: int):\n",
      "    return {\"id\": item_id, \"name\": \"Item \" + str(item_id), \"price\": 10.99}\n",
      "\n",
      "# Create a route to create a new item\n",
      "@app.post(\"/items/\")\n",
      "async def create_item(item: Item):\n",
      "    return {\"id\": 3, \"name\": item.name, \"price\": item.price}\n",
      "\n",
      "# Create a route to update an existing item\n",
      "@app.put(\"/items/{item_id}\")\n",
      "async def update_item(item_id: int, item: Item):\n",
      "    return {\"id\": item_id, \"name\": item.name, \"price\": item.price}\n",
      "\n",
      "# Create a route to delete an item\n",
      "@app.delete(\"/items/{item_id}\")\n",
      "async def delete_item(item_id: int):\n",
      "    return {\"message\": \"Item \" + str(item_id) + \" deleted\"}\n",
      "```\n",
      "\n",
      "### Running the Code\n",
      "\n",
      "To run the code, execute the following command in your terminal:\n",
      "\n",
      "```bash\n",
      "uvicorn app:app --reload\n",
      "```\n",
      "\n",
      "You can now access the API endpoints by visiting `http://localhost:8000/docs` in your web browser. The `--reload` flag allows the server to automatically reload when you make changes to the code.\n",
      "\n",
      "### Example Responses\n",
      "\n",
      "Here are some example responses from the API:\n",
      "\n",
      "* `GET /items/`: Returns a list of items.\n",
      "* `GET /items/1`: Returns a specific item by ID.\n",
      "* `POST /items/`: Creates a new item.\n",
      "* `PUT /items/1`: Updates an existing item.\n",
      "* `DELETE /items/1`: Deletes an item.\n",
      "\n",
      "Remember to replace the item IDs with actual values for updating and deleting items.\n",
      "Response Object: ChatCompletion(id='chatcmpl-aa8f106d-cd3f-4560-a9df-292843d5a4b6', choices=[Choice(finish_reason='stop', index=0, message=ChoiceMessage(role='assistant', content='Let\\'s break down the code step by step.\\n\\n**Section 1: Installation**\\n\\nThis section tells you how to install the required packages using pip.\\n\\n```bash\\npip install fastapi uvicorn\\n```\\n\\n* `pip` is the package manager for Python.\\n* `fastapi` is the framework we\\'re using to create the API.\\n* `uvicorn` is the ASGI server that hosts our API.\\n\\n**Section 2: Code**\\n\\nThis section defines the FastAPI application.\\n\\n```python\\n# app.py\\n\\nfrom fastapi import FastAPI\\nfrom pydantic import BaseModel\\n\\n# Create a FastAPI instance\\napp = FastAPI()\\n```\\n\\n* We\\'re importing `FastAPI` and `BaseModel` from their respective packages.\\n* `FastAPI` is initialized with the `app` attribute, which represents our API instance.\\n\\n**Section 3: Defining a Model**\\n\\nThis section defines a Pydantic model for data validation.\\n\\n```python\\n# Define a model for data validation\\nclass Item(BaseModel):\\n    name: str\\n    price: float\\n    is_active: bool\\n```\\n\\n* We\\'re creating a `BaseModel` subclass called `Item`.\\n* The `Item` class defines three properties: `name`, `price`, and `is_active`.\\n* These properties have data types (`str`, `float`, and `bool`).\\n* This model is used for data validation in our API endpoints.\\n\\n**Section 4: API Endpoints**\\n\\nThis section defines several API endpoints.\\n\\n```python\\n# Create a route to get a list of items\\n@app.get(\"/items/\")\\nasync def read_items():\\n    return [{\"id\": 1, \"name\": \"Item 1\", \"price\": 10.99}, {\"id\": 2, \"name\": \"Item 2\", \"price\": 5.99}]\\n\\n# Create a route to get a specific item by ID\\n@app.get(\"/items/{item_id}\")\\nasync def read_item(item_id: int):\\n    return {\"id\": item_id, \"name\": \"Item \" + str(item_id), \"price\": 10.99}\\n\\n# Create a route to create a new item\\n@app.post(\"/items/\")\\nasync def create_item(item: Item):\\n    return {\"id\": 3, \"name\": item.name, \"price\": item.price}\\n\\n# Create a route to update an existing item\\n@app.put(\"/items/{item_id}\")\\nasync def update_item(item_id: int, item: Item):\\n    return {\"id\": item_id, \"name\": item.name, \"price\": item.price}\\n\\n# Create a route to delete an item\\n@app.delete(\"/items/{item_id}\")\\nasync def delete_item(item_id: int):\\n    return {\"message\": \"Item \" + str(item_id) + \" deleted\"}\\n```\\n\\nLet\\'s break down each endpoint:\\n\\n1. `GET /items/`: This route returns a list of items.\\n\\n   * The `read_items` function returns a list of dictionaries.\\n   * Each dictionary has `id`, `name`, and `price` keys.\\n\\n2. `GET /items/{item_id}`: This route returns a specific item by ID.\\n\\n   * The `read_item` function takes an `item_id` parameter, which is an integer.\\n   * The function returns a dictionary with `id`, `name`, and `price` keys.\\n\\n3. `POST /items/`: This route creates a new item.\\n\\n   * The `create_item` function takes an `item` parameter, which is an instance of the `Item` model.\\n   * The function returns a dictionary with `id`, `name`, and `price` keys.\\n\\n4. `PUT /items/{item_id}`: This route updates an existing item.\\n\\n   * The `update_item` function takes an `item_id` parameter, which is an integer, and an `item` parameter, which is an instance of the `Item` model.\\n   * The function returns a dictionary with `id`, `name`, and `price` keys.\\n\\n5. `DELETE /items/{item_id}`: This route deletes an item.\\n\\n   * The `delete_item` function takes an `item_id` parameter, which is an integer.\\n   * The function returns a dictionary with a `message` key.\\n\\n**Section 5: Running the Code**\\n\\nThis section tells you how to run the code using Uvicorn.\\n\\n```bash\\nuvicorn app:app --reload\\n```\\n\\n* `uvicorn` is the ASGI server that hosts our API.\\n* `app:app` tells Uvicorn to load the `app` module and create an instance of the `FastAPI` app.\\n* `--reload` tells Uvicorn to automatically reload when you make changes to the code.\\n\\n**Section 6: Example Responses**\\n\\nThis section provides example responses from the API.\\n\\n* `GET /items/`: Returns a list of items.\\n* `GET /items/1`: Returns a specific item by ID.\\n* `POST /items/`: Creates a new item.\\n* `PUT /items/1`: Updates an existing item.\\n* `DELETE /items/1`: Deletes an item.', tool_calls=None), logprobs=None)], created=1737920904, model='llama3.1-8b', object='chat.completion', system_fingerprint='fp_6381a6c109', time_info=TimeInfo(completion_time=0.487055626, prompt_time=0.040047947, queue_time=0.00012747, total_time=0.52858567237854, created=1737920904), usage=Usage(completion_tokens=1063, prompt_tokens=718, total_tokens=1781), service_tier=None)\n",
      "AI: Let's break down the code step by step.\n",
      "\n",
      "**Section 1: Installation**\n",
      "\n",
      "This section tells you how to install the required packages using pip.\n",
      "\n",
      "```bash\n",
      "pip install fastapi uvicorn\n",
      "```\n",
      "\n",
      "* `pip` is the package manager for Python.\n",
      "* `fastapi` is the framework we're using to create the API.\n",
      "* `uvicorn` is the ASGI server that hosts our API.\n",
      "\n",
      "**Section 2: Code**\n",
      "\n",
      "This section defines the FastAPI application.\n",
      "\n",
      "```python\n",
      "# app.py\n",
      "\n",
      "from fastapi import FastAPI\n",
      "from pydantic import BaseModel\n",
      "\n",
      "# Create a FastAPI instance\n",
      "app = FastAPI()\n",
      "```\n",
      "\n",
      "* We're importing `FastAPI` and `BaseModel` from their respective packages.\n",
      "* `FastAPI` is initialized with the `app` attribute, which represents our API instance.\n",
      "\n",
      "**Section 3: Defining a Model**\n",
      "\n",
      "This section defines a Pydantic model for data validation.\n",
      "\n",
      "```python\n",
      "# Define a model for data validation\n",
      "class Item(BaseModel):\n",
      "    name: str\n",
      "    price: float\n",
      "    is_active: bool\n",
      "```\n",
      "\n",
      "* We're creating a `BaseModel` subclass called `Item`.\n",
      "* The `Item` class defines three properties: `name`, `price`, and `is_active`.\n",
      "* These properties have data types (`str`, `float`, and `bool`).\n",
      "* This model is used for data validation in our API endpoints.\n",
      "\n",
      "**Section 4: API Endpoints**\n",
      "\n",
      "This section defines several API endpoints.\n",
      "\n",
      "```python\n",
      "# Create a route to get a list of items\n",
      "@app.get(\"/items/\")\n",
      "async def read_items():\n",
      "    return [{\"id\": 1, \"name\": \"Item 1\", \"price\": 10.99}, {\"id\": 2, \"name\": \"Item 2\", \"price\": 5.99}]\n",
      "\n",
      "# Create a route to get a specific item by ID\n",
      "@app.get(\"/items/{item_id}\")\n",
      "async def read_item(item_id: int):\n",
      "    return {\"id\": item_id, \"name\": \"Item \" + str(item_id), \"price\": 10.99}\n",
      "\n",
      "# Create a route to create a new item\n",
      "@app.post(\"/items/\")\n",
      "async def create_item(item: Item):\n",
      "    return {\"id\": 3, \"name\": item.name, \"price\": item.price}\n",
      "\n",
      "# Create a route to update an existing item\n",
      "@app.put(\"/items/{item_id}\")\n",
      "async def update_item(item_id: int, item: Item):\n",
      "    return {\"id\": item_id, \"name\": item.name, \"price\": item.price}\n",
      "\n",
      "# Create a route to delete an item\n",
      "@app.delete(\"/items/{item_id}\")\n",
      "async def delete_item(item_id: int):\n",
      "    return {\"message\": \"Item \" + str(item_id) + \" deleted\"}\n",
      "```\n",
      "\n",
      "Let's break down each endpoint:\n",
      "\n",
      "1. `GET /items/`: This route returns a list of items.\n",
      "\n",
      "   * The `read_items` function returns a list of dictionaries.\n",
      "   * Each dictionary has `id`, `name`, and `price` keys.\n",
      "\n",
      "2. `GET /items/{item_id}`: This route returns a specific item by ID.\n",
      "\n",
      "   * The `read_item` function takes an `item_id` parameter, which is an integer.\n",
      "   * The function returns a dictionary with `id`, `name`, and `price` keys.\n",
      "\n",
      "3. `POST /items/`: This route creates a new item.\n",
      "\n",
      "   * The `create_item` function takes an `item` parameter, which is an instance of the `Item` model.\n",
      "   * The function returns a dictionary with `id`, `name`, and `price` keys.\n",
      "\n",
      "4. `PUT /items/{item_id}`: This route updates an existing item.\n",
      "\n",
      "   * The `update_item` function takes an `item_id` parameter, which is an integer, and an `item` parameter, which is an instance of the `Item` model.\n",
      "   * The function returns a dictionary with `id`, `name`, and `price` keys.\n",
      "\n",
      "5. `DELETE /items/{item_id}`: This route deletes an item.\n",
      "\n",
      "   * The `delete_item` function takes an `item_id` parameter, which is an integer.\n",
      "   * The function returns a dictionary with a `message` key.\n",
      "\n",
      "**Section 5: Running the Code**\n",
      "\n",
      "This section tells you how to run the code using Uvicorn.\n",
      "\n",
      "```bash\n",
      "uvicorn app:app --reload\n",
      "```\n",
      "\n",
      "* `uvicorn` is the ASGI server that hosts our API.\n",
      "* `app:app` tells Uvicorn to load the `app` module and create an instance of the `FastAPI` app.\n",
      "* `--reload` tells Uvicorn to automatically reload when you make changes to the code.\n",
      "\n",
      "**Section 6: Example Responses**\n",
      "\n",
      "This section provides example responses from the API.\n",
      "\n",
      "* `GET /items/`: Returns a list of items.\n",
      "* `GET /items/1`: Returns a specific item by ID.\n",
      "* `POST /items/`: Creates a new item.\n",
      "* `PUT /items/1`: Updates an existing item.\n",
      "* `DELETE /items/1`: Deletes an item.\n",
      "Response Object: ChatCompletion(id='chatcmpl-fd139175-5956-4877-94cb-7f6538012414', choices=[Choice(finish_reason='stop', index=0, message=ChoiceMessage(role='assistant', content='To better understand the code, let\\'s break it down into its core components.\\n\\n**Overview**\\n\\nThe code is a simple FastAPI application that provides endpoints for creating, reading, updating, and deleting items. The application uses Pydantic models for data validation and uses Uvicorn as the ASGI server.\\n\\n**Importing Libraries**\\n\\nThe code starts by importing the necessary libraries:\\n```python\\nfrom fastapi import FastAPI\\nfrom pydantic import BaseModel\\n```\\n* `FastAPI` is the framework used for building the API.\\n* `BaseModel` is a class from Pydantic that is used for data validation.\\n\\n**Creating the Application**\\n\\nThe code creates a new FastAPI application instance:\\n```python\\napp = FastAPI()\\n```\\n* This line creates a new instance of the `FastAPI` class and assigns it to the `app` variable.\\n\\n**Defining the Model**\\n\\nThe code defines a Pydantic model for data validation:\\n```python\\nclass Item(BaseModel):\\n    name: str\\n    price: float\\n    is_active: bool\\n```\\n* This line defines a new class called `Item` that inherits from `BaseModel`.\\n* The class has three attributes: `name`, `price`, and `is_active`.\\n* These attributes are annotated with their respective data types (`str`, `float`, and `bool`).\\n\\n**Defining the Endpoints**\\n\\nThe code defines several endpoints for the API:\\n```python\\n@app.get(\"/items/\")\\n@app.get(\"/items/{item_id}\")\\n@app.post(\"/items/\")\\n@app.put(\"/items/{item_id}\")\\n@app.delete(\"/items/{item_id}\")\\n```\\n* These lines define the routes for the API endpoints.\\n* Each endpoint is defined with a specific HTTP method (`GET`, `POST`, `PUT`, or `DELETE`).\\n* Each endpoint has a route specified as a string (e.g., \"/items/\" or \"/items/{item_id}\").\\n* The `{item_id}` part is a variable that will be passed as a parameter when the endpoint is called.\\n\\n**Endpoint Implementations**\\n\\nThe code provides the implementation for each endpoint:\\n```python\\nasync def read_items():\\n    return [{\"id\": 1, \"name\": \"Item 1\", \"price\": 10.99}, {\"id\": 2, \"name\": \"Item 2\", \"price\": 5.99}]\\n```\\n* This line defines a new function called `read_items` that returns a list of dictionaries.\\n* Each dictionary has three keys: `id`, `name`, and `price`.\\n\\nSimilarly, there are implementations for the other endpoints:\\n```python\\nasync def read_item(item_id: int):\\n    return {\"id\": item_id, \"name\": \"Item \" + str(item_id), \"price\": 10.99}\\n\\nasync def create_item(item: Item):\\n    return {\"id\": 3, \"name\": item.name, \"price\": item.price}\\n\\nasync def update_item(item_id: int, item: Item):\\n    return {\"id\": item_id, \"name\": item.name, \"price\": item.price}\\n\\nasync def delete_item(item_id: int):\\n    return {\"message\": \"Item \" + str(item_id) + \" deleted\"}\\n```\\n* These lines define new functions for each endpoint.\\n* Each function takes parameters (e.g., `item_id` or `item`) and returns a dictionary or message.\\n\\n**Running the Application**\\n\\nFinally, the code provides a way to run the application:\\n```bash\\nuvicorn app:app --reload\\n```\\n* This line runs the `uvicorn` server and loads the `app` module.\\n* The `--reload` flag tells `uvicorn` to automatically reload when changes are made to the code.\\n\\nThat\\'s a high-level overview of the code! Let me know if you have any specific questions about any of the components.', tool_calls=None), logprobs=None)], created=1737920915, model='llama3.1-8b', object='chat.completion', system_fingerprint='fp_6381a6c109', time_info=TimeInfo(completion_time=0.362286503, prompt_time=0.093601926, queue_time=8.497e-05, total_time=0.45772814750671387, created=1737920915), usage=Usage(completion_tokens=791, prompt_tokens=1788, total_tokens=2579), service_tier=None)\n",
      "AI: To better understand the code, let's break it down into its core components.\n",
      "\n",
      "**Overview**\n",
      "\n",
      "The code is a simple FastAPI application that provides endpoints for creating, reading, updating, and deleting items. The application uses Pydantic models for data validation and uses Uvicorn as the ASGI server.\n",
      "\n",
      "**Importing Libraries**\n",
      "\n",
      "The code starts by importing the necessary libraries:\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "from pydantic import BaseModel\n",
      "```\n",
      "* `FastAPI` is the framework used for building the API.\n",
      "* `BaseModel` is a class from Pydantic that is used for data validation.\n",
      "\n",
      "**Creating the Application**\n",
      "\n",
      "The code creates a new FastAPI application instance:\n",
      "```python\n",
      "app = FastAPI()\n",
      "```\n",
      "* This line creates a new instance of the `FastAPI` class and assigns it to the `app` variable.\n",
      "\n",
      "**Defining the Model**\n",
      "\n",
      "The code defines a Pydantic model for data validation:\n",
      "```python\n",
      "class Item(BaseModel):\n",
      "    name: str\n",
      "    price: float\n",
      "    is_active: bool\n",
      "```\n",
      "* This line defines a new class called `Item` that inherits from `BaseModel`.\n",
      "* The class has three attributes: `name`, `price`, and `is_active`.\n",
      "* These attributes are annotated with their respective data types (`str`, `float`, and `bool`).\n",
      "\n",
      "**Defining the Endpoints**\n",
      "\n",
      "The code defines several endpoints for the API:\n",
      "```python\n",
      "@app.get(\"/items/\")\n",
      "@app.get(\"/items/{item_id}\")\n",
      "@app.post(\"/items/\")\n",
      "@app.put(\"/items/{item_id}\")\n",
      "@app.delete(\"/items/{item_id}\")\n",
      "```\n",
      "* These lines define the routes for the API endpoints.\n",
      "* Each endpoint is defined with a specific HTTP method (`GET`, `POST`, `PUT`, or `DELETE`).\n",
      "* Each endpoint has a route specified as a string (e.g., \"/items/\" or \"/items/{item_id}\").\n",
      "* The `{item_id}` part is a variable that will be passed as a parameter when the endpoint is called.\n",
      "\n",
      "**Endpoint Implementations**\n",
      "\n",
      "The code provides the implementation for each endpoint:\n",
      "```python\n",
      "async def read_items():\n",
      "    return [{\"id\": 1, \"name\": \"Item 1\", \"price\": 10.99}, {\"id\": 2, \"name\": \"Item 2\", \"price\": 5.99}]\n",
      "```\n",
      "* This line defines a new function called `read_items` that returns a list of dictionaries.\n",
      "* Each dictionary has three keys: `id`, `name`, and `price`.\n",
      "\n",
      "Similarly, there are implementations for the other endpoints:\n",
      "```python\n",
      "async def read_item(item_id: int):\n",
      "    return {\"id\": item_id, \"name\": \"Item \" + str(item_id), \"price\": 10.99}\n",
      "\n",
      "async def create_item(item: Item):\n",
      "    return {\"id\": 3, \"name\": item.name, \"price\": item.price}\n",
      "\n",
      "async def update_item(item_id: int, item: Item):\n",
      "    return {\"id\": item_id, \"name\": item.name, \"price\": item.price}\n",
      "\n",
      "async def delete_item(item_id: int):\n",
      "    return {\"message\": \"Item \" + str(item_id) + \" deleted\"}\n",
      "```\n",
      "* These lines define new functions for each endpoint.\n",
      "* Each function takes parameters (e.g., `item_id` or `item`) and returns a dictionary or message.\n",
      "\n",
      "**Running the Application**\n",
      "\n",
      "Finally, the code provides a way to run the application:\n",
      "```bash\n",
      "uvicorn app:app --reload\n",
      "```\n",
      "* This line runs the `uvicorn` server and loads the `app` module.\n",
      "* The `--reload` flag tells `uvicorn` to automatically reload when changes are made to the code.\n",
      "\n",
      "That's a high-level overview of the code! Let me know if you have any specific questions about any of the components.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize Cerebras client\n",
    "api_key = os.environ.get(\"CEREBRAS_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"CEREBRAS_API_KEY environment variable is not set.\")\n",
    "\n",
    "client = Cerebras(api_key=api_key)\n",
    "\n",
    "# Initialize LangChain memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Function to interact with Cerebras API\n",
    "def get_cerebras_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"llama3.1-8b\",\n",
    "    )\n",
    "    # Debug: Print the entire response to inspect its structure\n",
    "    print(\"Response Object:\", response)\n",
    "    \n",
    "    # Adjust this based on the actual response structure\n",
    "    # Example: If response.choices[0].message.content is the correct path\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Create a conversation chain\n",
    "def chat():\n",
    "    print(\"Welcome to the Cerebras Chat! Type '/exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"/exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Add user input to memory\n",
    "        memory.chat_memory.add_user_message(user_input)\n",
    "\n",
    "        # Get the conversation history from memory\n",
    "        history = memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "        # Generate a prompt with conversation history\n",
    "        prompt = f\"{history}\\nUser: {user_input}\\nAI:\"\n",
    "\n",
    "        # Get response from Cerebras API\n",
    "        ai_response = get_cerebras_response(prompt)\n",
    "\n",
    "        # Add AI response to memory\n",
    "        memory.chat_memory.add_ai_message(ai_response)\n",
    "\n",
    "        # Print AI response\n",
    "        print(f\"AI: {ai_response}\")\n",
    "\n",
    "# Start the chat\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import Prompts  # Import the system prompt\n",
    "\n",
    "# Initialize Cerebras client\n",
    "api_key = os.environ.get(\"CEREBRAS_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"CEREBRAS_API_KEY environment variable is not set.\")\n",
    "\n",
    "client = Cerebras(api_key=api_key)\n",
    "\n",
    "# Initialize LangChain memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Add the system prompt to memory at the start\n",
    "memory.chat_memory.add_ai_message(Prompts.SYSTEM_PROMPT)\n",
    "\n",
    "# Function to interact with Cerebras API\n",
    "def get_cerebras_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": Prompts.SYSTEM_PROMPT},  # Use the imported system prompt\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=\"llama3.1-8b\",\n",
    "    )\n",
    "    # Debug: Print the entire response to inspect its structure\n",
    "    print(\"Response Object:\", response)\n",
    "    \n",
    "    # Adjust this based on the actual response structure\n",
    "    # Example: If response.choices[0].message.content is the correct path\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Create a conversation chain\n",
    "def chat():\n",
    "    print(\"Welcome to the Cerebras Chat! Type '/exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"/exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Add user input to memory\n",
    "        memory.chat_memory.add_user_message(user_input)\n",
    "\n",
    "        # Get the conversation history from memory\n",
    "        history = memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "        # Generate a prompt with conversation history\n",
    "        prompt = f\"{history}\\nUser: {user_input}\\nAI:\"\n",
    "\n",
    "        # Get response from Cerebras API\n",
    "        ai_response = get_cerebras_response(prompt)\n",
    "\n",
    "        # Add AI response to memory\n",
    "        memory.chat_memory.add_ai_message(ai_response)\n",
    "\n",
    "        # Print AI response\n",
    "        print(f\"AI: {ai_response}\")\n",
    "\n",
    "# Start the chat\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07adeed1c1e4af9b9a00ba02a271d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbba218b76944ec186c08727bd1da9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8194589e85964b80a5b838edad01f740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620a1d20b14e45d4b1ccf5d7f3c6b75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb61e5bf9924d00870cef0875efacff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1fa3cb1c7149c69f1b844fad87ec1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6112fa74961443738e182d429719c4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468d4f2f588b45a9bb8e1f407681b4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b8080f8b6c498a89b9fe0e469c07ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3bd3c00c414d9b982d04499a96e8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d2fce8328941f39e5d391d54196630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 embedding: [ 8.06947052e-03 -2.71037910e-02  1.46680363e-02  2.53898632e-02\n",
      "  8.83517340e-02  8.36232826e-02 -9.71423090e-03 -2.15860885e-02\n",
      "  1.32945910e-01  7.30283419e-03  5.35557568e-02  4.74516749e-02\n",
      " -7.58148283e-02  1.85709894e-02  2.10473891e-02 -7.62794316e-02\n",
      "  4.90425862e-02  2.82031316e-02 -1.04074053e-01 -1.06027097e-01\n",
      " -7.64405206e-02 -7.09755868e-02  3.54747591e-03 -5.77596501e-02\n",
      "  1.23879053e-02 -2.61637568e-02 -7.29246736e-02  5.60407620e-03\n",
      "  4.58459519e-02 -7.74129927e-02  3.91115667e-03  8.99396017e-02\n",
      "  5.18182442e-02 -2.04544906e-02 -7.04014301e-02  4.11856100e-02\n",
      "  1.38570555e-02  4.05999869e-02  5.35294414e-02  7.36391246e-02\n",
      " -1.86293647e-02 -8.58724350e-04 -6.17122278e-03  1.55163156e-02\n",
      "  4.34640869e-02 -1.53533416e-02  5.85178770e-02  6.05299473e-02\n",
      " -2.31038909e-02 -8.63915868e-03 -8.17053542e-02  5.16521484e-02\n",
      " -3.93878669e-02 -3.19278315e-02  2.04254128e-02  5.16047403e-02\n",
      "  2.42191944e-02  4.90597971e-02 -1.19914291e-02 -2.75306050e-02\n",
      " -4.21181954e-02 -1.11020632e-01 -3.30524263e-03  1.78532004e-02\n",
      "  7.53944218e-02 -2.57733203e-02  3.16136405e-02 -6.19211700e-03\n",
      "  5.62698953e-02  6.29182309e-02  1.82468183e-02  7.74442255e-02\n",
      " -3.53413224e-02  4.18320969e-02 -3.90374451e-04 -6.83688670e-02\n",
      "  4.93237413e-02 -3.66552621e-02  3.04249264e-02 -4.83291559e-02\n",
      "  1.06306998e-02 -6.12100028e-02 -3.22473794e-02  1.78529823e-03\n",
      "  5.32433242e-02  1.75530538e-02 -3.15081887e-02 -3.46865365e-03\n",
      " -1.13536835e-01 -3.77590619e-02  1.05809212e-01 -1.01879045e-01\n",
      " -6.57123793e-03 -2.19477974e-02  6.09051622e-02  6.50010258e-02\n",
      " -1.69193968e-02 -4.00170162e-02 -1.88187975e-02  5.78450449e-02\n",
      "  7.69446790e-02  6.94634393e-02  5.89137943e-03  3.91313694e-02\n",
      "  2.36069728e-02 -1.23300597e-01  9.29857716e-02  1.87984537e-02\n",
      "  1.39296185e-02 -2.72403043e-02 -1.88228134e-02  6.86533302e-02\n",
      "  4.53459844e-02  8.78711492e-02 -5.20198941e-02 -7.28678927e-02\n",
      "  4.03974624e-03 -1.58182345e-02  1.06050866e-02 -4.41285446e-02\n",
      " -7.36863315e-02  2.03438494e-02 -1.29479701e-02  7.55247101e-03\n",
      "  8.44024122e-02 -8.16529617e-03 -7.57977441e-02 -3.59557494e-33\n",
      " -1.89766157e-02 -1.21203056e-02  1.50072034e-02 -7.85963424e-03\n",
      " -1.18002377e-01  2.48854291e-02 -1.49891069e-02 -2.23724674e-02\n",
      "  3.31519581e-02  4.77766572e-03 -2.97697205e-02 -4.56381813e-02\n",
      "  3.34927328e-02  6.49021566e-02  1.21342950e-02  1.97464135e-02\n",
      " -9.54908878e-02  5.67870885e-02 -3.85719053e-02 -2.14462895e-02\n",
      "  3.64831164e-02 -6.54289275e-02 -1.91989599e-03 -7.77750323e-03\n",
      " -3.30076329e-02 -2.12149229e-02  5.94109483e-02  1.40233403e-02\n",
      " -2.45640278e-02  1.34762572e-02 -8.89517218e-02  2.43277736e-02\n",
      " -1.06212549e-01 -4.09439020e-02  5.15920855e-02 -5.42284027e-02\n",
      "  7.68834399e-03 -1.38621107e-01  2.87991147e-02 -1.32773900e-02\n",
      " -7.07072839e-02 -6.30758936e-03  4.98333797e-02 -3.69558632e-02\n",
      " -5.29422350e-02 -3.12265055e-03 -1.00915499e-01  1.69257801e-02\n",
      " -2.95510348e-02  4.52801622e-02  5.40383123e-02  1.32159302e-02\n",
      "  2.77080406e-02 -5.28459586e-02  3.89220305e-02  4.78483364e-02\n",
      " -1.44532938e-02 -2.32853182e-02  1.37476781e-02  6.96245804e-02\n",
      " -5.67237362e-02 -2.12819241e-02 -6.34259060e-02  6.54972121e-02\n",
      "  2.02745199e-02  1.20086104e-01 -6.58935234e-02  3.83517370e-02\n",
      "  7.40241483e-02  7.99327064e-03  2.15780847e-02 -9.53883026e-03\n",
      " -4.75126132e-02 -1.43555954e-01  6.37321025e-02  2.46852338e-02\n",
      " -4.44055535e-02  1.63817406e-03  4.43521934e-03 -4.90125641e-03\n",
      " -1.31144822e-02 -2.16365326e-02 -2.94105820e-02  1.49226785e-02\n",
      "  1.97178870e-02  1.22140730e-02  6.48485199e-02 -8.21918398e-02\n",
      "  1.62188355e-02 -3.20304930e-02  1.22166127e-02  3.56850624e-02\n",
      " -5.10328002e-02  1.14881163e-02 -5.32699600e-02  3.31041397e-33\n",
      " -1.23894140e-02 -5.67148961e-02  7.44952215e-03  7.60870501e-02\n",
      " -1.07244002e-02  1.55478930e-02 -7.10281804e-02 -9.16419476e-02\n",
      " -2.61557978e-02 -7.19008446e-02 -2.73745600e-02  2.57135741e-02\n",
      " -1.83592867e-02 -1.09266723e-02  8.89508799e-02  7.86578562e-03\n",
      "  2.07516141e-02 -5.61349504e-02  2.17537731e-02 -2.55144518e-02\n",
      "  7.22667128e-02  1.07414965e-02 -5.09067252e-02 -4.33505736e-02\n",
      " -1.80581994e-02  4.23613377e-02 -7.26958364e-02 -1.34375328e-02\n",
      " -5.25932722e-02  1.71753485e-02 -6.22480251e-02  1.50735257e-02\n",
      " -8.49033073e-02 -4.51033860e-02  6.28255424e-04  4.98211943e-02\n",
      "  2.13074666e-02  4.83573414e-02 -7.31610972e-03  2.81065870e-02\n",
      "  1.76691897e-02  2.22247094e-02 -6.44706003e-03 -8.24469924e-02\n",
      " -4.35049348e-02  6.76167160e-02  5.04048094e-02  4.45572250e-02\n",
      "  6.37177452e-02  1.34373218e-01  8.15863982e-02  8.19238797e-02\n",
      "  3.17554846e-02  5.02869301e-02  4.05935422e-02 -2.41758525e-02\n",
      " -8.95907879e-02 -8.51956382e-02 -1.03676980e-02  6.51961789e-02\n",
      " -5.57299703e-02  2.68832482e-02 -1.93236116e-02  8.82696360e-02\n",
      "  3.07446662e-02 -1.20288422e-02 -2.22920044e-03 -9.56161171e-02\n",
      "  8.32519606e-02 -4.47293743e-02  3.67974937e-02 -1.19334273e-02\n",
      " -3.13295126e-02  7.40488693e-02 -6.54894710e-02  1.89859830e-02\n",
      "  9.64899547e-03  2.26725508e-02 -6.74939528e-02  4.31209318e-02\n",
      " -1.24089420e-02 -7.45395795e-02  2.47405414e-02 -3.54687758e-02\n",
      " -5.28562665e-02  1.06409108e-02 -2.86488961e-02 -4.68839817e-02\n",
      "  8.10530409e-02  2.29134560e-02 -4.48944792e-02  2.19204333e-02\n",
      "  3.48816961e-02  2.86782943e-02 -9.95296985e-02 -1.35034002e-08\n",
      " -1.53723741e-02  7.48229725e-03  5.73709756e-02  2.26271823e-02\n",
      "  3.42125632e-02 -7.89044891e-03 -4.10394147e-02  1.21251784e-01\n",
      " -5.97231314e-02  1.05143450e-02  6.05763085e-02 -1.64696481e-02\n",
      " -1.14046568e-02  1.93210505e-02  7.93456063e-02  2.32348163e-02\n",
      "  9.36238468e-02 -7.83902258e-02 -4.37328778e-02 -2.23718639e-02\n",
      "  3.47350240e-02  3.05312201e-02 -6.79648966e-02  2.10052896e-02\n",
      "  2.90792026e-02  2.47065779e-02  7.44675025e-02  8.84488672e-02\n",
      "  1.30015481e-02 -1.03556830e-02 -6.95170537e-02  5.59753589e-02\n",
      " -1.12601500e-02  2.78440900e-02  8.25623274e-02  4.47923392e-02\n",
      "  4.15645493e-03  3.78014073e-02  3.86243872e-02 -6.83168024e-02\n",
      " -5.79507612e-02  3.16622145e-02 -3.66699211e-02  4.15234715e-02\n",
      "  3.81918363e-02 -4.02981602e-02  2.51273103e-02 -1.85562316e-02\n",
      " -2.49831714e-02  7.16537386e-02 -1.52032729e-02  1.94031298e-02\n",
      "  4.13540564e-02 -1.89089365e-02 -1.69283152e-02  1.06651120e-01\n",
      "  6.46185204e-02 -1.14366241e-01 -5.76222986e-02  1.06596462e-02\n",
      "  7.52112120e-02  5.13524786e-02 -5.56386029e-03 -6.38641641e-02]\n",
      "Sentence 2 embedding: [ 8.95442888e-02  6.33151904e-02  6.21951483e-02  1.16200514e-01\n",
      "  3.01151890e-02 -2.56184582e-02  7.99164549e-02 -9.45408735e-03\n",
      "  3.84116620e-02 -1.81699991e-02  1.04989633e-01 -3.04359291e-02\n",
      "  6.07844070e-02 -3.36755551e-02  3.31454575e-02  1.08422467e-03\n",
      "  9.58960317e-03 -5.71114905e-02 -1.11494005e-01  3.55417579e-02\n",
      "  3.36622037e-02 -5.96037321e-03  5.90290017e-02  3.65010463e-02\n",
      " -2.48841848e-02 -1.52350813e-02 -4.63293530e-02  9.56397355e-02\n",
      "  1.20375402e-01 -4.03451559e-04 -3.41343954e-02 -2.62731090e-02\n",
      " -1.68285556e-02  5.43867163e-02  1.04722353e-02  5.50953783e-02\n",
      " -1.06357113e-02  7.85374939e-02 -4.69274074e-02 -3.11769024e-02\n",
      "  1.87677499e-02 -3.19716521e-02  1.37809524e-03 -1.18178837e-02\n",
      "  1.23158451e-02 -9.05166492e-02  1.34696057e-02  4.94835945e-03\n",
      "  7.08690332e-03 -6.65989220e-02 -8.54782164e-02 -3.15609723e-02\n",
      " -1.05176777e-01 -8.91656987e-03  2.27057990e-02  7.41353035e-02\n",
      " -4.72944900e-02  6.41575828e-02  1.01578515e-02 -4.69317921e-02\n",
      "  3.71106826e-02 -1.68069787e-02 -9.24905688e-02  8.31785798e-02\n",
      "  1.41056001e-01 -3.10485605e-02  5.86284790e-03  2.62242109e-02\n",
      " -1.35102391e-01  1.33386686e-01  3.17566507e-02  2.67927684e-02\n",
      " -5.34822866e-02 -4.11196984e-03 -8.28235000e-02 -5.50441779e-02\n",
      "  3.79064903e-02 -7.94372801e-03  8.82060900e-02  1.90643705e-02\n",
      " -9.22034606e-02 -8.53757635e-02 -1.92679521e-02  4.95493747e-02\n",
      "  4.22534682e-02 -1.23223169e-02  2.85512162e-03 -1.15400292e-01\n",
      " -1.88675839e-02  3.02215163e-02 -3.75734307e-02 -1.00085512e-01\n",
      "  4.93035540e-02 -1.96636654e-02 -8.76391958e-03 -1.34350397e-02\n",
      " -5.29077910e-02 -2.79305261e-02 -1.85851492e-02  1.07457176e-01\n",
      " -1.93184696e-03  7.21037760e-02  3.79935279e-02  3.20885219e-02\n",
      " -5.46437427e-02 -8.14000666e-02 -3.12095825e-02 -6.72261119e-02\n",
      " -2.44218134e-03 -2.71677468e-02 -8.08603168e-02 -1.27658648e-02\n",
      " -7.47398846e-03 -1.57978460e-02 -8.36870912e-03 -4.03635167e-02\n",
      " -4.70289364e-02  9.98595357e-03 -1.02936894e-01  3.15901116e-02\n",
      "  3.35728079e-02  6.42786920e-02 -5.10859974e-02  3.15575153e-02\n",
      " -4.75549586e-02 -8.11494440e-02  1.05264515e-01 -6.29316329e-33\n",
      "  2.68271230e-02 -9.01127160e-02 -2.02478394e-02 -3.63911060e-03\n",
      "  4.68992181e-02 -5.32817356e-02 -3.46311182e-02  6.79379562e-04\n",
      " -2.73320470e-02  4.18232046e-02 -7.33128004e-03 -2.39167865e-02\n",
      "  2.00645942e-02 -8.95374082e-03  2.77272109e-02  6.62213191e-02\n",
      " -8.08822140e-02  1.09312944e-01  1.59188714e-02  5.86359901e-03\n",
      "  5.09453937e-03  6.01995401e-02 -2.36867536e-02  6.43096399e-03\n",
      " -2.65936181e-02 -5.23469672e-02  4.13400419e-02 -5.95746888e-03\n",
      " -2.55583800e-05  3.66899148e-02  1.13206990e-02  3.64839919e-02\n",
      " -6.53288746e-03  3.42744850e-02  1.07837163e-01  6.37464300e-02\n",
      "  6.55247867e-02 -2.01439839e-02  6.48355670e-03 -1.81962810e-02\n",
      " -3.20863910e-02 -2.67344061e-02  3.07242982e-02 -3.73159349e-02\n",
      "  1.01004206e-01  3.72202578e-03  1.70367751e-02  4.06894311e-02\n",
      " -6.43334985e-02  1.82227250e-02  6.52394909e-03  7.25939870e-02\n",
      "  5.92804998e-02 -1.30884964e-02  8.78348723e-02 -9.39545501e-03\n",
      "  2.11864952e-02  7.31568187e-02  2.47258134e-02  2.65483483e-04\n",
      " -2.74339560e-02  5.10366224e-02 -4.37096208e-02  7.99780339e-02\n",
      " -4.59546000e-02  1.02277752e-02  1.98462349e-03 -7.02515990e-02\n",
      "  1.31028052e-02  3.28812785e-02 -4.36971476e-03  5.26935654e-03\n",
      " -1.13522470e-01  5.88741601e-02 -3.97416092e-02 -1.62651334e-02\n",
      " -3.56591083e-02 -4.03967798e-02 -1.82002671e-02  5.09506615e-04\n",
      " -1.43064233e-02 -1.44784749e-01 -4.93093859e-03  4.47248667e-02\n",
      " -3.41752656e-02  4.41969819e-02  6.86774999e-02 -7.18600824e-02\n",
      " -2.10838951e-03 -1.53738598e-03 -2.60609612e-02  3.57950386e-03\n",
      "  5.56586236e-02  1.02755968e-02 -2.61275265e-02  2.93978703e-33\n",
      " -8.13012347e-02  3.26794572e-02 -8.11210573e-02  4.45402972e-02\n",
      "  8.35569426e-02  1.47555135e-02  1.97542626e-02 -1.09067716e-01\n",
      "  1.04704406e-02 -5.43533126e-04 -7.85709694e-02  7.07509229e-04\n",
      "  4.75026518e-02 -1.22434339e-02  3.34425410e-03 -2.45790109e-02\n",
      "  7.82281384e-02  5.74129336e-02 -1.42411338e-02  6.11540638e-02\n",
      " -4.82368022e-02  7.51392767e-02 -3.49833034e-02  6.73077926e-02\n",
      " -5.64675182e-02  2.29946449e-02 -2.69391201e-02 -1.24050968e-03\n",
      " -1.39665797e-01 -5.22578508e-02  1.93636790e-02 -7.96944425e-02\n",
      " -5.17132618e-02  1.01856794e-03 -4.01254371e-02  6.72603101e-02\n",
      "  5.84222488e-02 -6.34038001e-02 -8.89365152e-02  3.05630062e-02\n",
      "  4.42606620e-02 -3.23072486e-02  6.73068175e-03  1.19073093e-01\n",
      "  1.00536318e-03 -5.55455424e-02 -2.17594728e-02 -8.32559094e-02\n",
      "  7.86625966e-03  3.12507413e-02 -1.08322658e-01  3.20649408e-02\n",
      " -6.38293549e-02 -1.49284052e-02 -7.38714859e-02 -2.39046719e-02\n",
      " -2.15046983e-02 -5.33570424e-02 -4.57918504e-03 -4.71510785e-03\n",
      " -4.13760468e-02  1.40578458e-02 -4.07670550e-02  4.82772179e-02\n",
      "  8.03822875e-02 -7.88709074e-02 -6.02534972e-02  3.19540426e-02\n",
      "  5.41330762e-02  6.44467995e-02  5.20074293e-02 -4.30352129e-02\n",
      " -8.34599137e-02 -1.53567716e-02 -1.15574099e-01  1.06923487e-02\n",
      " -6.78534508e-02 -3.88530269e-02 -6.87401071e-02 -3.42635438e-02\n",
      "  2.36781277e-02 -2.68657133e-02  1.66536048e-02 -3.17965169e-03\n",
      " -4.49151173e-03 -5.57386018e-02  8.00706260e-03 -9.99189988e-02\n",
      " -2.74544973e-02  1.15289455e-02 -1.34854987e-02 -3.36609818e-02\n",
      "  3.03216465e-02  6.32815883e-02  3.30020953e-03 -1.69632113e-08\n",
      " -2.85392553e-02  1.52481705e-04  6.88581541e-02 -3.43456529e-02\n",
      "  3.82560827e-02 -3.91234457e-02  4.73904125e-02 -3.54055725e-02\n",
      "  1.07271727e-02 -4.36124429e-02  3.87428477e-02  3.04528531e-02\n",
      " -4.93599102e-02  7.86929484e-03  2.41100304e-02  8.15291926e-02\n",
      " -2.75747607e-05 -2.45438777e-02 -2.22123880e-02  4.83108573e-02\n",
      " -4.17661518e-02  5.62024489e-02 -4.65798005e-02  4.27827565e-03\n",
      "  2.84014978e-02 -5.45799173e-03  2.16675866e-02  6.70240372e-02\n",
      "  9.91625525e-03  3.30365659e-03  6.49207681e-02  1.06206730e-01\n",
      " -1.39241554e-02 -2.26403540e-03  5.66933826e-02  2.08013561e-02\n",
      "  5.79740405e-02 -4.96043870e-03  4.63217907e-02 -5.79508627e-03\n",
      "  2.46939939e-02  3.40028293e-02 -9.12418868e-03  6.02049828e-02\n",
      "  6.94703609e-02  4.31092680e-02 -2.53474060e-03 -5.93826473e-02\n",
      " -5.92609635e-03  2.63700206e-02 -2.71264138e-03  4.29753065e-02\n",
      "  2.47985553e-02 -1.11348815e-02  5.88551052e-02  6.94650859e-02\n",
      "  7.40926415e-02 -3.06030083e-03 -6.88051209e-02 -1.17358519e-02\n",
      "  8.15442577e-03 -1.49828545e-03  3.34632508e-02 -3.89461517e-02]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for a list of sentences\n",
    "sentences = [\"Why is fast inference important?\", \"Another example sentence.\"]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Print the embeddings\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    print(f\"Sentence {i+1} embedding: {embedding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "\n",
    "# =======================\n",
    "# Configuration\n",
    "# =======================\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\", \"us-west1-gcp\")\n",
    "PINECONE_INDEX_NAME = \"text-vector-store-3f9f8e1e-9d6a-4a5b-8c2d-2f1e3d4b5c6a\"  # Your specific index name\n",
    "\n",
    "CEREBRAS_API_KEY = os.getenv(\"CEREBRAS_API_KEY\")\n",
    "CEREBRAS_MODEL = \"llama3.1-8b\"\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "\n",
    "# Initialize Cerebras client\n",
    "cerebras_client = Cerebras(api_key=CEREBRAS_API_KEY)\n",
    "\n",
    "# Initialize embeddings (OpenAI or any other compatible embeddings)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Load the specific Pinecone index\n",
    "if PINECONE_INDEX_NAME not in pinecone.list_indexes():\n",
    "    raise ValueError(f\"Pinecone index '{PINECONE_INDEX_NAME}' does not exist.\")\n",
    "\n",
    "vector_store = Pinecone.from_existing_index(PINECONE_INDEX_NAME, embeddings)\n",
    "\n",
    "# Initialize conversation memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# =======================\n",
    "# Custom LLM Wrapper for Cerebras\n",
    "# =======================\n",
    "class CerebrasLLM:\n",
    "    def __init__(self, client, model):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=self.model,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Initialize Cerebras LLM\n",
    "cerebras_llm = CerebrasLLM(cerebras_client, CEREBRAS_MODEL)\n",
    "\n",
    "# =======================\n",
    "# RAG Pipeline with Conversation Memory\n",
    "# =======================\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=cerebras_llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# Chat Function\n",
    "# =======================\n",
    "def chat(query):\n",
    "    try:\n",
    "        # Get response from RAG pipeline\n",
    "        result = qa_chain({\"question\": query})\n",
    "        return result[\"answer\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# =======================\n",
    "# Example Usage\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to the RAG Chat! Type '/exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"/exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = chat(user_input)\n",
    "        print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response Body: {'index_name': 'vecstored1166eb468c7', 'dimension': 1536, 'metadata': {'index_name': 'vecstored1166eb468c7', 'environment': 'us-east-1', 'dimension': 1536, 'vector_count': 1}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:8000/create-vector-store\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"text\": \"Your text goes here. This could be any text you want to convert into a vector store.\",\n",
    "    \"chunk_size\": 500,\n",
    "    \"overlap\": 50\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Body:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response Body: {'message': \"Pinecone index 'vecstored1166eb468c7' has been deleted successfully.\", 'index_name': 'vecstored1166eb468c7'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:8000/delete-vector-store\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"index_name\": \"vecstored1166eb468c7\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Body:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2463506640.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    openai migrate\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "openai migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
